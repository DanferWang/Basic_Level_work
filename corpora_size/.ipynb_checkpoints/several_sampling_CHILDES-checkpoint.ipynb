{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# only once run for download WordNet or update\n",
    "import nltk\n",
    "# nltk.download('wordnet', download_dir='./')\n",
    "nltk.data.path.append('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the abnormal result of CHILDES by 50 times sampling corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load sub-corporas\n",
    "brown = pylangacq.read_chat(\"./corpora/CHILDES/Brown.zip\")\n",
    "belf = pylangacq.read_chat(\"./corpora/CHILDES/Belfast.zip\")\n",
    "crutt=pylangacq.read_chat(\"./corpora/CHILDES/Cruttenden.zip\")\n",
    "fletcher=pylangacq.read_chat(\"./corpora/CHILDES/Fletcher.zip\")\n",
    "forr=pylangacq.read_chat(\"./corpora/CHILDES/Forrester.zip\")\n",
    "gath=pylangacq.read_chat(\"./corpora/CHILDES/Gathburn.zip\")\n",
    "howe=pylangacq.read_chat(\"./corpora/CHILDES/Howe.zip\")\n",
    "kelly=pylangacq.read_chat(\"./corpora/CHILDES/KellyQuigley.zip\")\n",
    "korman=pylangacq.read_chat(\"./corpora/CHILDES/Korman.zip\")\n",
    "lara=pylangacq.read_chat(\"./corpora/CHILDES/Lara.zip\")\n",
    "manc=pylangacq.read_chat(\"./corpora/CHILDES/Manchester.zip\")\n",
    "nuff=pylangacq.read_chat(\"./corpora/CHILDES/Nuffield.zip\")\n",
    "quigley=pylangacq.read_chat(\"./corpora/CHILDES/QuigleyMcNally.zip\")\n",
    "sekali=pylangacq.read_chat(\"./corpora/CHILDES/Sekali.zip\")\n",
    "smith=pylangacq.read_chat(\"./corpora/CHILDES/Smith.zip\")\n",
    "tommer=pylangacq.read_chat(\"./corpora/CHILDES/Tommerdahl.zip\")\n",
    "\n",
    "# link up sub-corporas\n",
    "corpora = [brown, belf, crutt, fletcher, forr, gath, howe, kelly, korman, lara, manc, nuff, quigley, sekali, smith, tommer]\n",
    "reader = pylangacq.Reader()\n",
    "for item in corpora:\n",
    "    reader.append(item)\n",
    "\n",
    "# preparation sample for 5.7m, 2.4m, 1m\n",
    "words = reader.words()\n",
    "childes_df = pd.Series(words)\n",
    "random_seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inherit features from Gold Standard dataset\n",
    "GS_all_agreed = pd.read_csv('./sampled_count/GS_All_Agreed.csv', index_col=0)\n",
    "features_target = ['Synsets','domain_x',\n",
    "                   'nrdirhypers_x',\n",
    "                   'nrhypos_x',\n",
    "                   'nrpartrels_normalised_x',\n",
    "                   'depthfromtopsynset_normalised_x',\n",
    "                   'glosslength_normalised_x',\n",
    "                   'minwordlength_x',\n",
    "                   'nroflemmas_x',\n",
    "                   'polyscore_max_x',\n",
    "                   'vote_x']\n",
    "GS_adopt = GS_all_agreed[features_target]\n",
    "\n",
    "# extract norms from synsets\n",
    "GS_adopt['norm'] = GS_adopt['Synsets'].str.split(\"'\").str[1].str.split('.').str[0]\n",
    "GS_adopt = GS_adopt.set_index('norm').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# matching norms with corpora\n",
    "def sum_lemmas(norm, corpora):\n",
    "    # search norm in WordNet\n",
    "    synsets_list = wordnet.synsets(norm)\n",
    "    lemmas = []\n",
    "    feq_count = 0\n",
    "    for synset in synsets_list:\n",
    "        # extract lemmas from every synset\n",
    "        lemmas += [str(lemma.name()) for lemma in synset.lemmas()]\n",
    "    for lemma in lemmas:\n",
    "        # check each lemma in corpora\n",
    "        feq_count += corpora.loc[lemma == corpora.index].to_numpy().sum()\n",
    "\n",
    "    return feq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define features and target\n",
    "features = ['nrdirhypers_x',\n",
    "            'nrhypos_x',\n",
    "            'nrpartrels_normalised_x',\n",
    "            'depthfromtopsynset_normalised_x',\n",
    "            'glosslength_normalised_x',\n",
    "            'minwordlength_x',\n",
    "            'nroflemmas_x',\n",
    "            'polyscore_max_x']\n",
    "target = ['vote_x'] # nb / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split training set and testing set using K-Flod\n",
    "def global_model_test(dataset, feature, sized_corpora, target):\n",
    "    K = 10\n",
    "    random_seed = 7 # R\n",
    "    data = dataset.reset_index()\n",
    "    feature_list = feature + [sized_corpora]\n",
    "    X = data[feature_list]\n",
    "    y = data[target]\n",
    "\n",
    "    K_Flod = StratifiedKFold(n_splits=K, shuffle=True, random_state=random_seed)\n",
    "    K_Flod.get_n_splits(X, y)\n",
    "    cohen_kappa = []\n",
    "    balanced_acc = []\n",
    "    for train_index, test_index in K_Flod.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # SMOTE algorithm\n",
    "        smote = SMOTE(random_state=random_seed, k_neighbors=2)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # define random forest model\n",
    "        rf = RandomForestClassifier(random_state=random_seed, max_features='sqrt', n_estimators=1400, min_samples_split=2, min_samples_leaf=1, max_depth=50, oob_score=True, criterion='gini', bootstrap=True).fit(X_train, y_train)\n",
    "\n",
    "        # predict and make score\n",
    "        pipeline = make_pipeline(smote, rf)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "        cohen_kappa.append(kappa)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        balanced_acc.append(balanced_accuracy)\n",
    "\n",
    "    results = classification_report(y_test, y_pred, output_dict=True)\n",
    "    results = pd.DataFrame(results).transpose()\n",
    "\n",
    "    results['cohen kappa / 10'] = np.mean(cohen_kappa)\n",
    "    results['balanced acc / 10'] = np.mean(balanced_acc)\n",
    "    results['global'] = 5\n",
    "\n",
    "    # importance of features\n",
    "    # importance = rf.feature_importances_\n",
    "    # importance = pd.DataFrame([features, importance]).transpose()\n",
    "    # importance = importance.rename(columns={0:'feature', 1:'importance'}).sort_values('importance', ascending=False)\n",
    "\n",
    "    importance = pd.DataFrame() # no importance\n",
    "    return results, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to perform several times sampling\n",
    "def multiSample(n):\n",
    "    result_dict = {}\n",
    "    for time in range(n):\n",
    "        childes_5_7m = childes_df.sample(n=5700000, replace=True)\n",
    "        childes_2_4m = childes_df.sample(n=2400000)\n",
    "        childes_1m = childes_df.sample(n=1000000)\n",
    "\n",
    "        # create a word count from dictionary into dataframe\n",
    "        # addition: convert all words into low case\n",
    "        childes_5_7m_freq = nltk.FreqDist(word.lower() for word in childes_5_7m)\n",
    "        childes_5_7m_df = pd.DataFrame.from_dict(childes_5_7m_freq, orient='index').reset_index().rename(columns={0: 'CHILDES_Count', 'index':'norm'}).set_index('norm')\n",
    "        childes_2_4m_freq = nltk.FreqDist(word.lower() for word in childes_2_4m)\n",
    "        childes_2_4m_df = pd.DataFrame.from_dict(childes_2_4m_freq, orient='index').reset_index().rename(columns={0: 'CHILDES_Count', 'index':'norm'}).set_index('norm')\n",
    "        childes_1m_freq = nltk.FreqDist(word.lower() for word in childes_1m)\n",
    "        childes_1m_df = pd.DataFrame.from_dict(childes_1m_freq, orient='index').reset_index().rename(columns={0: 'CHILDES_Count', 'index':'norm'}).set_index('norm')\n",
    "\n",
    "        # childes_rel_sum: The sum of all instances of each lemma per synset in the CHILDES corpus, devided by the total number of words in the corpus\n",
    "        total_count_1m = childes_1m_df['CHILDES_Count'].sum()\n",
    "        GS_adopt['childes_1m_rel_sum'] = GS_adopt['norm'].apply(lambda norm: sum_lemmas(norm, childes_1m_df)/total_count_1m)\n",
    "        total_count_2_4m = childes_2_4m_df['CHILDES_Count'].sum()\n",
    "        GS_adopt['childes_2_4m_rel_sum'] = GS_adopt['norm'].apply(lambda norm: sum_lemmas(norm, childes_2_4m_df)/total_count_2_4m)\n",
    "        total_count_5_7m = childes_5_7m_df['CHILDES_Count'].sum()\n",
    "        GS_adopt['childes_5_7m_rel_sum'] = GS_adopt['norm'].apply(lambda norm: sum_lemmas(norm, childes_5_7m_df)/total_count_5_7m)\n",
    "\n",
    "        # GlobalModel test\n",
    "        result_dict[time] = {'1M':{}, '2_4M':{}, '5_7M':{}}\n",
    "        sized_corpora = 'childes_1m_rel_sum'\n",
    "        result_kappa_acc, result_importance = global_model_test(GS_adopt, features, sized_corpora, target)\n",
    "        result_dict[time]['1M']['kappa']  = result_kappa_acc['cohen kappa / 10'][0]\n",
    "        result_dict[time]['1M']['balanced acc'] = result_kappa_acc['balanced acc / 10'][0]\n",
    "        sized_corpora = 'childes_2_4m_rel_sum'\n",
    "        result_kappa_acc, result_importance = global_model_test(GS_adopt, features, sized_corpora, target)\n",
    "        result_dict[time]['2_4M']['kappa']  = result_kappa_acc['cohen kappa / 10'][0]\n",
    "        result_dict[time]['2_4M']['balanced acc'] = result_kappa_acc['balanced acc / 10'][0]\n",
    "        sized_corpora = 'childes_5_7m_rel_sum'\n",
    "        result_kappa_acc, result_importance = global_model_test(GS_adopt, features, sized_corpora, target)\n",
    "        result_dict[time]['5_7M']['kappa']  = result_kappa_acc['cohen kappa / 10'][0]\n",
    "        result_dict[time]['5_7M']['balanced acc'] = result_kappa_acc['balanced acc / 10'][0]\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multiResults = multiSample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.save('50_times_sampling_CHILDES.npy', multiResults, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'1M': {'kappa': 0.6497156300400995, 'balanced acc': 0.8326016973953762},\n",
       "  '2_4M': {'kappa': 0.6786945952062448, 'balanced acc': 0.8473853992071726},\n",
       "  '5_7M': {'kappa': 0.6842012075111802, 'balanced acc': 0.8488892436214648}}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiResults = np.load('50_times_sampling_CHILDES.npy', allow_pickle=True)\n",
    "multiResults.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# kappa\n",
    "x = ['1M', '2.4M', '5.7M']\n",
    "fig, axs = plt.subplots(10, 2, figsize=(18,52))\n",
    "fig.suptitle('kappa: 10 times sampling for CHILDES', y=0.89)\n",
    "for time in range(10):\n",
    "    sub_dict = multiResults[time]\n",
    "    kappa = [sub_dict['1M']['results']['cohen kappa / 10'][0], sub_dict['2_4M']['results']['cohen kappa / 10'][0], sub_dict['5_7M']['results']['cohen kappa / 10'][0]]\n",
    "    acc = [sub_dict['1M']['results']['balanced acc / 10'][0], sub_dict['2_4M']['results']['balanced acc / 10'][0], sub_dict['5_7M']['results']['balanced acc / 10'][0]]\n",
    "    axs[time, 0].plot(x, kappa, marker='.', color='#61DDAA')\n",
    "    axs[time, 0].set(xlabel='Corpora size', ylabel='kappa', title=str(time+1))\n",
    "    axs[time, 1].plot(x, acc, marker='.', color='#61DDAA')\n",
    "    axs[time, 1].set(xlabel='Corpora size', ylabel='balanced accuracy', title=str(time+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multiResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125479/1881623605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmultiResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'multiResults' is not defined"
     ]
    }
   ],
   "source": [
    "multiResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
